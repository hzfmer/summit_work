{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import resample, resample_poly\n",
    "import struct\n",
    "import imageio\n",
    "import collections\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib2 import Path\n",
    "import pretty_errors\n",
    "from filter_BU import filt_B\n",
    "import my_pyrotd\n",
    "from awp_processing import awp\n",
    "from post_processing.la_habra import *\n",
    "\n",
    "np.errstate(divide='ignore')\n",
    "#%config InlineBackend.figure_format = 'retina'\n",
    "# %matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.signal.tf_misfit import plot_tfr, em, pm, tem, tpm, fem, fpm, tfem, tfpm\n",
    "\n",
    "def resize(vel, vel_rec, dt=None):\n",
    "    length = int(len(vel['t']) * vel['dt'] // dt)\n",
    "    length_rec = int(len(vel_rec['t']) * vel_rec['dt'] // dt)\n",
    "    #print(f'Length of time steps = {length}')\n",
    "    if length > len(vel['t']):\n",
    "        print(f\"Upsample may introduce alias. Use larger dt_sample instead.\\n\")\n",
    "        return\n",
    "    for comp in 'XYZ':\n",
    "        if length > length_rec:     \n",
    "            vel[comp] = vel[comp][vel['t'] <= vel_rec['t'][-1]]\n",
    "            vel[comp] = resample(vel[comp], length_rec)\n",
    "            vel_rec[comp] = resample(vel_rec[comp], length_rec)\n",
    "        else:\n",
    "            vel_rec[comp] = vel_rec[comp][vel_rec['t'] <= vel['t'][-1]]\n",
    "            vel_rec[comp] = resample(vel_rec[comp], length)\n",
    "            vel[comp] = resample(vel[comp], length)\n",
    "            \n",
    "   \n",
    "def plot_obspy_tf_misfit(misfit, comps='XYZ', left=0.1, bottom=0.1,\n",
    "                    h_1=0.2, h_2=0.125, h_3=0.2, w_1=0.2, w_2=0.6, w_cb=0.01,\n",
    "                    d_cb=0.0, show=True, plot_args=['k', 'r', 'b'], ylim=0.,\n",
    "                    clim=0., cmap='RdBu_r', fmin=0.15, fmax=5, dpi=300):\n",
    "    from matplotlib.ticker import NullFormatter\n",
    "    figs = []\n",
    "    t = misfit['t']\n",
    "    f = misfit['f']\n",
    "    ntr = len(comps)\n",
    "    for itr, comp in enumerate(comps):\n",
    "        fig = plt.figure(dpi=dpi)\n",
    "        data = misfit[comp]\n",
    "        \n",
    "        # plot signals\n",
    "        ax_sig = fig.add_axes([left + w_1, bottom + h_2 + h_3, w_2, h_1])\n",
    "        ax_sig.plot(t, data['rec'], plot_args[0], label=f'data, max={100 * np.max(data[\"rec\"]):.3f} cm/s')\n",
    "        ax_sig.plot(t, data['syn'], plot_args[1], label=f'syn, max={100 * np.max(data[\"syn\"]):.3f} cm/s')\n",
    "        ax_sig.legend(loc=1, ncol=2)\n",
    "\n",
    "        # plot TEM\n",
    "        if 'tem' in data:\n",
    "            ax_tem = fig.add_axes([left + w_1, bottom + h_1 + h_2 + h_3, w_2, h_2])\n",
    "            ax_tem.plot(t, data['tem'], plot_args[2])\n",
    "\n",
    "        # plot TFEM\n",
    "        if 'tfem' in data:\n",
    "            ax_tfem = fig.add_axes([left + w_1, bottom + h_1 + 2 * h_2 + h_3, w_2,\n",
    "                                    h_3])\n",
    "\n",
    "            img_tfem = ax_tfem.pcolormesh(t, f, data['tfem'], cmap=cmap)\n",
    "            img_tfem.set_rasterized(True)\n",
    "            ax_tfem.set_yscale(\"log\")\n",
    "            ax_tfem.set_ylim(fmin, fmax)\n",
    "\n",
    "        # plot FEM\n",
    "        if 'fem' in data:\n",
    "            ax_fem = fig.add_axes([left, bottom + h_1 + 2 * h_2 + h_3, w_1, h_3])\n",
    "            ax_fem.semilogy(data['fem'], f, plot_args[2])\n",
    "            ax_fem.set_ylim(fmin, fmax)\n",
    "\n",
    "        # plot TPM\n",
    "        if 'tpm' in data:\n",
    "            ax_tpm = fig.add_axes([left + w_1, bottom, w_2, h_2])\n",
    "            ax_tpm.plot(t, data['tpm'], plot_args[2])\n",
    "\n",
    "        # plot TFPM\n",
    "        if 'tfpm' in data:\n",
    "            ax_tfpm = fig.add_axes([left + w_1, bottom + h_2, w_2, h_3])\n",
    "\n",
    "            img_tfpm = ax_tfpm.pcolormesh(t, f, data['tfpm'], cmap=cmap)\n",
    "            img_tfpm.set_rasterized(True)\n",
    "            ax_tfpm.set_yscale(\"log\")\n",
    "            ax_tfpm.set_ylim(f[0], f[-1])\n",
    "\n",
    "        # add colorbars\n",
    "        ax_cb_tfpm = fig.add_axes([left + w_1 + w_2 + d_cb + w_cb, bottom,\n",
    "                                   w_cb, h_2 + h_3])\n",
    "        fig.colorbar(img_tfpm, cax=ax_cb_tfpm)\n",
    "\n",
    "        # plot FPM\n",
    "        if 'fpm' in data:\n",
    "            ax_fpm = fig.add_axes([left, bottom + h_2, w_1, h_3])\n",
    "            ax_fpm.semilogy(data['fpm'], f, plot_args[2])\n",
    "            ax_fpm.set_ylim(fmin, fmax)\n",
    "\n",
    "        # set limits\n",
    "        ylim_sig = np.max([np.abs(data['rec']).max(), np.abs(data['syn']).max()]) * 2\n",
    "        ax_sig.set_ylim(-ylim_sig, ylim_sig)\n",
    "\n",
    "        if ylim == 0.:\n",
    "            ylim = np.max([np.abs(data[key]).max() for key in ['tem', 'tpm', 'fem', 'fpm'] \\\n",
    "                           if key in data]) * 1.1\n",
    "    \n",
    "        ax_tem.set_ylim(-ylim, ylim)\n",
    "        ax_fem.set_xlim(-ylim, ylim)\n",
    "        ax_tpm.set_ylim(-ylim, ylim)\n",
    "        ax_fpm.set_xlim(-ylim, ylim)\n",
    "\n",
    "        ax_sig.set_xlim(t[0], t[-1])\n",
    "        ax_tem.set_xlim(t[0], t[-1])\n",
    "        ax_tpm.set_xlim(t[0], t[-1])\n",
    "\n",
    "        if clim == 0.:\n",
    "            clim = np.max([np.abs(data['tfem']).max(), np.abs(data['tfpm']).max()])\n",
    "\n",
    "        img_tfpm.set_clim(-clim, clim)\n",
    "        img_tfem.set_clim(-clim, clim)\n",
    "\n",
    "        # add text box for EM + PM\n",
    "        textstr = f\"{comp}-component\\nEM = {data['em']: .2f}\\nPM = {data['pm']: .2f}\"\n",
    "        props = dict(boxstyle='round', facecolor='white')\n",
    "        ax_sig.text(-0.3, 0.5, textstr, transform=ax_sig.transAxes,\n",
    "                    verticalalignment='center', horizontalalignment='left',\n",
    "                    bbox=props)\n",
    "\n",
    "        ax_tpm.set_xlabel('time (s)')\n",
    "        ax_fem.set_ylabel('frequency (Hz)')\n",
    "        ax_fpm.set_ylabel('frequency (Hz)')\n",
    "\n",
    "        # add text boxes\n",
    "        props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
    "        ax_tfem.text(0.95, 0.85, 'TFEM', transform=ax_tfem.transAxes,\n",
    "                     verticalalignment='top', horizontalalignment='right',\n",
    "                     bbox=props)\n",
    "        ax_tfpm.text(0.95, 0.85, 'TFPM', transform=ax_tfpm.transAxes,\n",
    "                     verticalalignment='top', horizontalalignment='right',\n",
    "                     bbox=props)\n",
    "        ax_tem.text(0.95, 0.75, 'TEM', transform=ax_tem.transAxes,\n",
    "                    verticalalignment='top', horizontalalignment='right',\n",
    "                    bbox=props)\n",
    "        ax_tpm.text(0.95, 0.75, 'TPM', transform=ax_tpm.transAxes,\n",
    "                    verticalalignment='top', horizontalalignment='right',\n",
    "                    bbox=props)\n",
    "        ax_fem.text(0.9, 0.85, 'FEM', transform=ax_fem.transAxes,\n",
    "                    verticalalignment='top', horizontalalignment='right',\n",
    "                    bbox=props)\n",
    "        ax_fpm.text(0.9, 0.85, 'FPM', transform=ax_fpm.transAxes,\n",
    "                    verticalalignment='top', horizontalalignment='right',\n",
    "                    bbox=props)\n",
    "\n",
    "        # remove axis labels\n",
    "        ax_tfpm.xaxis.set_major_formatter(NullFormatter())\n",
    "        ax_tfem.xaxis.set_major_formatter(NullFormatter())\n",
    "        ax_tem.xaxis.set_major_formatter(NullFormatter())\n",
    "        ax_sig.xaxis.set_major_formatter(NullFormatter())\n",
    "        ax_tfpm.yaxis.set_major_formatter(NullFormatter())\n",
    "        ax_tfem.yaxis.set_major_formatter(NullFormatter())\n",
    "\n",
    "        figs.append(fig)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        if ntr == 1:\n",
    "            return figs[0]\n",
    "        else:\n",
    "            return figs\n",
    "        \n",
    "        \n",
    "def comp_obspy_tf_misfit(model, site_name, dt, comps='XYZ', \n",
    "                         fmin=0.15, fmax=5, nf=128, vel=None, vel_rec=None, plot=False):\n",
    "    if vel is None:\n",
    "        with open('results/vel_syn.pickle', 'rb') as fid:\n",
    "            vel_syn = pickle.load(fid)\n",
    "        vel = vel_syn[model][site_name]\n",
    "        vel_rec = vel_syn[\"rec\"][site_name]\n",
    "        \n",
    "    resize(vel, vel_rec, dt)\n",
    "    res = {}\n",
    "    res['dt'] = dt\n",
    "    res['f'] = np.logspace(np.log10(fmin), np.log10(fmax), nf)\n",
    "    \n",
    "    for comp in comps:\n",
    "        res[comp] = {}\n",
    "        res[comp]['syn'] = vel[comp]\n",
    "        res[comp]['rec'] = vel_rec[comp]\n",
    "        res[comp]['em'] = em(vel[comp], vel_rec[comp], dt, fmin, fmax, nf)\n",
    "        res[comp]['pm'] = pm(vel[comp], vel_rec[comp], dt, fmin, fmax, nf)\n",
    "        res[comp]['tfem'] = tfem(vel[comp], vel_rec[comp], dt, fmin, fmax, nf)\n",
    "        res[comp]['tfpm'] = tfpm(vel[comp], vel_rec[comp], dt, fmin, fmax, nf)\n",
    "        res[comp]['tem'] = tem(vel[comp], vel_rec[comp], dt, fmin, fmax, nf)\n",
    "        res[comp]['tpm'] = tpm(vel[comp], vel_rec[comp], dt, fmin, fmax, nf)\n",
    "        res[comp]['fem'] = fem(vel[comp], vel_rec[comp], dt, fmin, fmax, nf)\n",
    "        res[comp]['fpm'] = fpm(vel[comp], vel_rec[comp], dt, fmin, fmax, nf)\n",
    "    res['t'] = np.arange(res[comp]['tem'].shape[-1]) * dt\n",
    "    if plot:\n",
    "        plot_obspy_tf_misfit(res, comps=comps)\n",
    "    return res\n",
    "        \n",
    "    \n",
    "    \n",
    "def comp_obspy_tf_misfits(models, dt, comps='XYZ', fmin=0.15, fmax=5, nf=128):\n",
    "    misfit = {}\n",
    "    with open('results/vel_syn.pickle', 'rb') as fid:\n",
    "        vel_syn = pickle.load(fid)\n",
    "        \n",
    "    for model in models:\n",
    "        misfit[model] = {}\n",
    "        vel = vel_syn[model]\n",
    "        vel_rec = vel_syn['rec']\n",
    "        for site_name in vel_syn[model].keys():\n",
    "            print(f'{model}: {site_name}')\n",
    "            misfit[model][site_name] = comp_obspy_tf_misfit( \\\n",
    "                    model, site_name, dt, comps=comps, fmin=fmin, fmax=fmax, \n",
    "                    nf=nf, vel=vel[site_name], vel_rec=vel_rec[site_name])\n",
    "    return misfit\n",
    "\n",
    "def comp_obspy_tf_misfit_short(model, site_name, dt, comps='XYZ', \n",
    "                         fmin=0.15, fmax=5, nf=128, vel=None, vel_rec=None, plot=False):\n",
    "    if vel is None:\n",
    "        with open('results/vel_syn.pickle', 'rb') as fid:\n",
    "            vel_syn = pickle.load(fid)\n",
    "        vel = vel_syn[model][site_name]\n",
    "        vel_rec = vel_syn[\"rec\"][site_name]\n",
    "        \n",
    "    resize(vel, vel_rec, dt)\n",
    "    res = {}\n",
    "    res['dt'] = dt\n",
    "    res['f'] = np.logspace(np.log10(fmin), np.log10(fmax), nf)\n",
    "    \n",
    "    for comp in comps:\n",
    "        res[comp] = {}\n",
    "        res[comp]['syn'] = vel[comp]\n",
    "        res[comp]['rec'] = vel_rec[comp]\n",
    "        res[comp]['em'] = em(vel[comp], vel_rec[comp], dt, fmin, fmax, nf)\n",
    "        res[comp]['pm'] = pm(vel[comp], vel_rec[comp], dt, fmin, fmax, nf)\n",
    "        res[comp]['eg'] = 10 * np.exp(-abs(res[comp]['em']))\n",
    "        res[comp]['pg'] = 10 * (1 - np.abs(res[comp]['pm']))\n",
    "    res['t'] = np.arange(len(res[comp]['syn'])) * dt\n",
    "    return res\n",
    "\n",
    "\n",
    "def comp_obspy_tf_misfits_short(models, dt, comps='XYZ', fmin=0.15, fmax=5, nf=128):\n",
    "    with open('results/vel_syn.pickle', 'rb') as fid:\n",
    "        vel_syn = pickle.load(fid)\n",
    "        \n",
    "    misfit = {}\n",
    "    for model in models:\n",
    "        misfit[model] = {}\n",
    "        vel = vel_syn[model]\n",
    "        vel_rec = vel_syn['rec']\n",
    "        for i, site_name in enumerate(vel_syn[model].keys()):\n",
    "            if i % 50 == 0:\n",
    "                print(f'{model}: {site_name}')\n",
    "            tmp = comp_obspy_tf_misfit_short( \\\n",
    "                    model, site_name, dt, comps=comps, fmin=fmin, fmax=fmax, \n",
    "                    nf=nf, vel=vel[site_name], vel_rec=vel_rec[site_name])\n",
    "            out = []\n",
    "            for i, comp in enumerate(comps):\n",
    "                out.append(tmp[comp]['em'])  \n",
    "                out.append(tmp[comp]['pm'])  \n",
    "            for i, comp in enumerate(comps):\n",
    "                out.append(tmp[comp]['eg'])  \n",
    "                out.append(tmp[comp]['pg'])\n",
    "            misfit[model][site_name] = np.array(out, dtype='float32').reshape(6, 2)\n",
    "    return misfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(fname):\n",
    "    return  pickle.loads(Path(fname).read_bytes())\n",
    "\n",
    "def write_pickle(data, fname):\n",
    "    with open(fname, 'wb') as fid:\n",
    "        pickle.dump(data, fid, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_topo(fname, mx, my):\n",
    "    \"\"\"Return topog (my, mx)\"\"\"\n",
    "    pad = 8\n",
    "    with open(fname, 'rb') as fout:\n",
    "        mx, my, pad = np.frombuffer(fout.read(12), dtype='int32')\n",
    "        topo = np.frombuffer(fout.read((mx + 2 * pad) * (my + 2 * pad) * 4),\n",
    "                             dtype='float32').reshape(mx + 2 * pad, my + 2 * pad).T\n",
    "        return topo[pad : -pad, pad : -pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5803, 3075, 284]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'distance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-03372534138b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0msite_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msyn_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     site_dist[site_name] = np.sqrt((srcidx[-1] * dh / 1000) ** 2 +  \\\n\u001b[0;32m---> 68\u001b[0;31m                            distance(site_latlon[i][1], site_latlon[i][2]) ** 2)\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0msite_vs30\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite_latlon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0msite_elev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msite_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopography\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msyn_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyn_sites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'distance' is not defined"
     ]
    }
   ],
   "source": [
    "# band pass filter\n",
    "lowf, highf = 0.15, 5\n",
    "osc_freqs = np.concatenate((np.linspace(0.1, 10, 100),\n",
    "                        [1/3, 2, 3, 4, 5]))\n",
    "osc_freqs = sorted(osc_freqs)\n",
    "fqs = [2, 3, 4, 5]\n",
    "\n",
    "\n",
    "mx, my = 19440, 14904\n",
    "dh = 8\n",
    "tmax, dt, tskip, wstep, nfile = read_param(\"\")\n",
    "tpad = tmax + 5  # Padding 5 seconds when using bbs preparing\n",
    "nd = 500  # avoid abc boudnary, which is at most 80 * 3 = 240\n",
    "dt = dt * tskip\n",
    "nt = int(tmax / dt)\n",
    "fs = 1 / dt\n",
    "\n",
    "# Topography\n",
    "if \"topography\" not in locals():\n",
    "    topography = read_topo('topography.bin', mx, my)\n",
    "    topo_grad = np.gradient(topography, dh)\n",
    "    topo_grad = np.sqrt(topo_grad[0] ** 2 + topo_grad[1] ** 2)\n",
    "    \n",
    "# sites of recordings\n",
    "# url_rec = 'http://hypocenter.usc.edu/bbp/highf/data/2020-08-10-data-processed/'\n",
    "# r = requests.get(url_rec)\n",
    "# rec_sites = re.findall('(?<=\">p-).+(?=\\\\.V2.vel)', r.text)\n",
    "\n",
    "# orig_sites = np.genfromtxt('stat_name_idx.txt', delimiter=\" \", dtype=\"S8, i4, i4\")\n",
    "# seems Fabio doesn't remove \"_\" in site name now\n",
    "if os.path.isfile('results/syn_sites.pickle'):\n",
    "    with open('results/syn_sites.pickle', 'rb') as fid:\n",
    "        syn_sites = pickle.load(fid)\n",
    "else:\n",
    "    syn_sites = [(name.decode('UTF-8').replace('_', ''), ix, iy) for name, ix, iy in orig_sites \\\n",
    "             if nd < ix < mx - nd and nd < iy < my - nd] \n",
    "    with open('results/syn_sites.pickle', 'wb') as fid:\n",
    "        pickle.dump(syn_sites, fid, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Source index \n",
    "if 1 and 'srcidx' not in locals():\n",
    "#     grids = np.fromfile('surf.grid', dtype='float64').reshape(my, mx, 3)[:, :, :2]\n",
    "#     grids_squeeze = np.reshape(grids, (-1, 2))\n",
    "    nsrcx = nsrcz = 125\n",
    "    src_lonlat = [-117.932587, 33.918633]\n",
    "    src_idx = np.genfromtxt('fault_idx.txt', dtype='int').reshape(nsrcz, nsrcx, 3)\n",
    "    srcidx = [int(x) for x in (np.mean(src_idx, axis=(0, 1)))]  # (ix, iy, iz)\n",
    "    \n",
    "    print(srcidx)\n",
    "    # from scipy import spatial\n",
    "    # kdtree = spatial.cKDTree(grids_squeeze)\n",
    "    # query_idx = np.unravel_index(kdtree.query(src_lonlat)[1], (my, mx))  # (iy, ix)\n",
    "    # srcidx = query_idx[::-1] + (srcidx[-1],)  #(ix, iy, iz)\n",
    "    # srcidx = [4037, 2732, 709]\n",
    "#     print(f'Interpolated source lon/lat: {grids[srcidx[1], srcidx[0]]}\\n', \n",
    "#           f'Close to records? {np.isclose(grids[srcidx[1], srcidx[0]], src_lonlat, atol=5e-5)}')\n",
    "#     del grids\n",
    "\n",
    "site_latlon = np.genfromtxt('la_habra_large_statlist_070120.txt', usecols=[0,1,2,3], dtype=\"S8, f, f, f\")\n",
    "site_dist = {}\n",
    "site_vs30 = {}\n",
    "site_elev = {}\n",
    "site_idx = {}\n",
    "for i in range(len(site_latlon)):\n",
    "    site_name = site_latlon[i][0].decode('UTF-8').replace('_', '')\n",
    "    site_idx[site_name] = (syn_sites[i][1], syn_sites[i][2])\n",
    "    site_dist[site_name] = np.sqrt((srcidx[-1] * dh / 1000) ** 2 +  \\\n",
    "                           distance(site_latlon[i][1], site_latlon[i][2]) ** 2)\n",
    "    site_vs30[site_name] = site_latlon[i][3]\n",
    "    site_elev[site_name] = topography[syn_sites[i][2], syn_sites[i][1]]\n",
    "\n",
    "sorted_site_elev = [(k, v) for k, v in sorted(site_elev.items(), key=lambda x: x[1])]\n",
    "sorted_site_dist = [(k, v) for k, v in sorted(site_dist.items(), key=lambda x: x[1])]\n",
    "\n",
    "\n",
    "  \n",
    "# # VS30 is at the approxmately 5th layer; skip = 4\n",
    "# from scipy.stats import hmean\n",
    "# vs30 = np.fromfile('mesh', dtype='float32', count=mx * my * 3 * 3).reshape(3, my, mx, 3)[:, :, :, 1]\n",
    "# vs30 = hmean(vs30, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = []\n",
    "for i in range(len(site_latlon)):\n",
    "    d = {}\n",
    "    site_name = site_latlon[i][0].decode('UTF-8').replace('_', '')\n",
    "    d[\"site_name\"] = site_name \n",
    "    d[\"lon\"] = site_latlon[i][1]\n",
    "    d[\"lat\"] = site_latlon[i][2]\n",
    "    d[\"idx_x\"] = syn_sites[i][1]\n",
    "    d[\"idx_y\"] = syn_sites[i][2]\n",
    "    d[\"vs30\"] = site_latlon[i][3]\n",
    "    d[\"elevation\"] = topography[syn_sites[i][2], syn_sites[i][1]]\n",
    "    d[\"rhyp\"] = site_dist[site_name]\n",
    "    ds.append(d)\n",
    "df_sites = pd.DataFrame(ds, index=None)\n",
    "#df_sites.set_index('site_name', drop=False, inplace=True)\n",
    "df_sites.sort_values(\"rhyp\", inplace=True)\n",
    "df_sites.reset_index(drop=True, inplace=True)\n",
    "#df_sites.to_csv(\"results/df_sites.csv\", index=False)\n",
    "df_sites.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "left, right, bot, top = -118.5, -117.23, 33.6, 34.35\n",
    "\n",
    "labels = {'arias': r'Arias (cm/s)', 'pga': r'PGA (cm/s@+2@+)', 'pgv': r'PGV (cm/s)',\n",
    "          'ener': r'ENER (cm/s@+2@+)', 'dur': 'DUR (s)'}\n",
    "metrics = list(labels.keys())\n",
    "\n",
    "model_dict = {\n",
    "    'rec': 'data',\n",
    "    'dhyp0.50_s1485839278_q100f00_orig_vs200': \"src0_vs200\",\n",
    "    'dhyp0.50_s1485839278_q100f00_orig_vs500': \"src0_vs500\",\n",
    "    'dhyp1.00_s387100462_q100f00_orig_vs200': \"src1_vs200\",\n",
    "    'dhyp1.50_s372823598_q100f00_orig_vs200': \"src2_vs200\",\n",
    "    'dhyp0.50_s1485839278_q100f00_s05h005l100_vs200': \"src0_s5h5l100vs200\",\n",
    "    'dhyp0.50_s1485839278_q100f00_s05h005l500_vs200': \"src0_s5h5l500vs200\",\n",
    "    'dhyp0.50_s1485839278_q100f00_s10h005l500_vs200': \"src0_s10h5l500vs200\",\n",
    "    'q100f00_orig_vs500': \"vs500\",\n",
    "    'q100f06_orig_vs500': \"q100f06_vs500\",\n",
    "    \"topo_q100f00_orig_vs500\": \"tp_vs500\",\n",
    "    'topo_q100f00_s05h005l100_vs500': \"tp_s5h5l100_vs500\",\n",
    "    'topo_q100f06_s05h005l100_vs500': \"tp_q100f06_s5h5l100_vs500\",\n",
    "    'topo_q50f06_s05h005l100_vs500': \"tp_q50f06_s5h5l100_vs500\",\n",
    "    'topo_q50f08_s05h005l100_vs500': \"tp_q50f08_s5h5l100_vs500\",\n",
    "}\n",
    "model_id = {k: i for i, k in enumerate(model_dict)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_syn = collections.defaultdict(dict)\n",
    "\n",
    "\n",
    "models = ['dhyp0.50_s1485839278_q100f00_orig_vs200',\n",
    "          'dhyp0.50_s1485839278_q100f00_orig_vs500',\n",
    "          'dhyp1.50_s372823598_q100f00_orig_vs200',\n",
    "          'dhyp1.00_s387100462_q100f00_orig_vs200',\n",
    "          'dhyp0.50_s1485839278_q100f00_s05h005l100_vs200',\n",
    "          'dhyp0.50_s1485839278_q100f00_s05h005l500_vs200',\n",
    "          'dhyp0.50_s1485839278_q100f00_s10h005l500_vs200',\n",
    "          'rec']\n",
    "\n",
    "with open('results/vel_syn.pickle', 'rb') as fid:\n",
    "    vel_syn = pickle.load(fid)\n",
    "\n",
    "\n",
    "# models = ['dhyp0.50_s1485839278_q100f00_orig_vs200']\n",
    "# for model in models:\n",
    "#     if model in vel_syn.keys():\n",
    "#         continue\n",
    "#     try:\n",
    "#         with open(Path(model, 'vel_sites.pickle'), 'rb') as fid:      \n",
    "#             vel_syn[model] = pickle.load(fid) \n",
    "#             for k in vel_syn[model].keys():  # Each site\n",
    "#                 vel_syn[model][k] = rotate(vel_syn[model][k], -39.9)\n",
    "#                 vel_syn[model][k] = prepare_bbpvel(vel_syn[model][k], tmax)\n",
    "\n",
    "#     except:\n",
    "#         print(\"No model found: \", model)\n",
    "#         with open(f'results/vel_{model}.pickle', 'rb') as fid:\n",
    "#             vel_syn[model] = pickle.load(fid)\n",
    "            \n",
    "# with open(f'results/vel_rec.pickle', 'rb') as fid:\n",
    "#     vel_syn['rec'] = pickle.load(fid)\n",
    "# # for k in vel_syn['rec'].keys():  # Each site\n",
    "# #     vel_syn['rec'][k] = prepare_bbpvel(vel_syn['rec'][k], tmax, shift=vel_syn['rec'][k]['shift'], \n",
    "# #                                        dt=vel_syn['dhyp0.50_s1485839278_q100f00_orig_vs200'][k]['dt'])\n",
    "\n",
    "# with open(Path(\"../la_habra_large_gpu_abc50/topo_q100f06_s05h005l100_vs500\", 'vel_sites.pickle'), 'rb') as fid:      \n",
    "#     tmp = pickle.load(fid) \n",
    "#     for k in tmp.keys():  # Each site\n",
    "#         tmp[k] = rotate(tmp[k], -39.9)\n",
    "#         tmp[k] = prepare_bbpvel(tmp[k], tmax)\n",
    "\n",
    "# vel_syn['topo_q100f06_s05h005l100_vs500'] = tmp\n",
    "\n",
    "# with open('results/vel_syn.pickle', 'wb') as fid:\n",
    "#     pickle.dump(vel_syn, fid, protocol=pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(Path(\"../la_habra_large_gpu_abc50/topo_q100f06_s05h005l100_vs500\", 'vel_sites.pickle'), 'rb') as fid:      \n",
    "#     tmp = pickle.load(fid) \n",
    "#     for k in tmp.keys():  # Each site\n",
    "#         tmp[k] = rotate(tmp[k], -39.9)\n",
    "#         tmp[k] = prepare_bbpvel(tmp[k], tmax)\n",
    "\n",
    "# vel_syn['topo_q100f06_s05h005l100_vs500'] = tmp\n",
    "# with open('results/vel_syn.pickle', 'wb') as fid:\n",
    "#     pickle.dump(vel_syn, fid, protocol=pickle.HIGHEST_PROTOCOL) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = ['dhyp0.50_s1485839278_q100f00_orig_vs200',\n",
    "#           'dhyp0.50_s1485839278_q100f00_orig_vs500',\n",
    "#           'dhyp1.50_s372823598_q100f00_orig_vs200',\n",
    "#           'dhyp1.00_s387100462_q100f00_orig_vs200',\n",
    "#           'dhyp0.50_s1485839278_q100f00_s05h005l100_vs200',\n",
    "#           'dhyp0.50_s1485839278_q100f00_s05h005l500_vs200',\n",
    "#           'dhyp0.50_s1485839278_q100f00_s10h005l500_vs200',\n",
    "#          ]\n",
    "# models = ['topo_q100f06_s05h005l100_vs500']\n",
    "\n",
    "# comp_metrics(models, vel_syn, lowcut=0.15, highcut=[0, 1, 2.5, 4, 5],\n",
    "#              tmax=tmax, force_update=1, save=True) \n",
    "# comp_metrics(models, vel_syn, lowcut=2.5, highcut=[5], tmax=tmax,\n",
    "#              force_update=1, save=True)  \n",
    "# models = list(met[(0.15, 1)].keys())\n",
    "# gof = comp_GOF([0, 1, 2.5, 4, 5, (2.5, 5)] , models, vs=site_vs30, \n",
    "#     syn_sites=syn_sites, topography=topography, sx=srcidx[0], sy=srcidx[1],\n",
    "#         sz=srcidx[2], dh=0.008)\n",
    "\n",
    "met = pickle.loads(Path('results/metrics.pickle').read_bytes())\n",
    "gof = pickle.loads(Path('results/gof.pickle').read_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge previous results if not present\n",
    "if 0:\n",
    "    tmp = read_pickle(\"../la_habra_large_gpu_abc50/results/vel_syn.pickle\")\n",
    "    vel = pickle.loads(Path('results/vel_syn.pickle').read_bytes())\n",
    "    if model not in vel:\n",
    "        vel[model] = tmp[model]\n",
    "    write_pickle(vel, 'results/vel_syn.pickle')\n",
    "\n",
    "    tmp = read_pickle(\"../la_habra_large_gpu_abc50/results/psa_syn.pickle\")\n",
    "    psa = pickle.loads(Path('results/psa_syn.pickle').read_bytes())\n",
    "    for model in tmp.keys():\n",
    "        if model not in psa:\n",
    "            psa[model] = tmp[model]\n",
    "    write_pickle(psa, 'results/psa_syn.pickle')\n",
    "\n",
    "    tmp = read_pickle(\"../la_habra_large_gpu_abc50/results/psax_syn.pickle\")\n",
    "    psa = pickle.loads(Path('results/psax_syn.pickle').read_bytes())\n",
    "    for model in tmp.keys():\n",
    "        if model not in psa:\n",
    "            psa[model] = tmp[model]\n",
    "    write_pickle(psa, 'results/psax_syn.pickle')\n",
    "\n",
    "    tmp = read_pickle(\"../la_habra_large_gpu_abc50/results/psay_syn.pickle\")\n",
    "    psa = pickle.loads(Path('results/psay_syn.pickle').read_bytes())\n",
    "    for model in tmp.keys():\n",
    "        if model not in psa:\n",
    "            psa[model] = tmp[model]\n",
    "    write_pickle(psa, 'results/psay_syn.pickle')\n",
    "\n",
    "\n",
    "    tmp = read_pickle(\"../la_habra_large_gpu_abc50/results/gof.pickle\")\n",
    "    gof = pickle.loads(Path('results/gof.pickle').read_bytes())\n",
    "    for f in tmp.keys():\n",
    "        for model in tmp[f].keys():\n",
    "            if f in gof and model not in gof[f]:\n",
    "                gof[f][model] = tmp[f][model]\n",
    "    write_pickle(gof, 'results/gof.pickle')\n",
    "\n",
    "\n",
    "    tmp = read_pickle(\"../la_habra_large_gpu_abc50/results/metrics.pickle\")\n",
    "    met = pickle.loads(Path('results/metrics.pickle').read_bytes())\n",
    "    for f in tmp.keys():\n",
    "        for model in tmp[f].keys():\n",
    "            if f in met and model not in met[f]:\n",
    "                met[f][model] = tmp[f][model]\n",
    "    write_pickle(met, 'results/metrics.pickle')\n",
    "\n",
    "    tmp = read_pickle(\"../la_habra_large_gpu_abc50/results/tf_misfit.pickle\")\n",
    "    tf_misfit = pickle.loads(Path('results/tf_misfit.pickle').read_bytes())\n",
    "    for f in tmp.keys():\n",
    "        for model in tmp[f].keys():\n",
    "            if f in tf_misfit and model not in tf_misfit[f]:\n",
    "                tf_misfit[f][model] = tmp[f][model]\n",
    "    write_pickle(tf_misfit, 'results/tf_misfit.pickle')\n",
    "\n",
    "    del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this one\n",
    "# EM/PM/EG/PG only tf_misfit\n",
    "\n",
    "dt = 0.04\n",
    "try:\n",
    "    tf_misfit = pickle.loads(Path('results/tf_misfit.pickle').read_bytes())\n",
    "except:\n",
    "    tf_misfit = {}\n",
    "    \n",
    "models = ['dhyp0.50_s1485839278_q100f00_orig_vs200',\n",
    "          'dhyp0.50_s1485839278_q100f00_orig_vs500',\n",
    "          'dhyp1.50_s372823598_q100f00_orig_vs200',\n",
    "          'dhyp1.00_s387100462_q100f00_orig_vs200',\n",
    "          'dhyp0.50_s1485839278_q100f00_s05h005l100_vs200',\n",
    "          'dhyp0.50_s1485839278_q100f00_s05h005l500_vs200',\n",
    "          'dhyp0.50_s1485839278_q100f00_s10h005l500_vs200']\n",
    "models = ['topo_q100f06_s05h005l100_vs500']\n",
    "tmp = {}\n",
    "for f in [(0.15, 2.5), (0.15, 5), (2.5, 5), (0.15, 1), (0.15, 4)]:\n",
    "    tf_misfit[f].pop('topo_q100f06_s05h005l100_vs500')\n",
    "    if f not in tf_misfit:\n",
    "        tf_misfit[f] = {}\n",
    "    models = [model for model in models if model not in tf_misfit[f]]\n",
    "    tmp = comp_obspy_tf_misfits_short(models, dt, fmin=f[0], fmax=f[1])\n",
    "    for model in models:\n",
    "        print(model)\n",
    "        tf_misfit[f][model] = tmp[model]\n",
    "print(tf_misfit[(0.15, 2.5)].keys())\n",
    "\n",
    "\n",
    "# # force rec tf_misfits\n",
    "# rec_tf_misfit = np.zeros((6, 2))\n",
    "# rec_tf_misfit[3:,:] = 10.\n",
    "# for f in tf_misfit.keys():\n",
    "#     tf_misfit[f]['rec'] = {}\n",
    "#     for site_name in tf_misfit[f][models[0]].keys():\n",
    "#         tf_misfit[f]['rec'][site_name] = rec_tf_misfit.copy()\n",
    "        \n",
    "with open('results/tf_misfit.pickle', 'wb') as fid:\n",
    "    pickle.dump(tf_misfit, fid, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Single tf_misfit for 0.15 - 5 Hz, with all components, fem, fpm ...\n",
    "\n",
    "\n",
    "# fmin, fmax = 0.15, 5\n",
    "# models = ['dhyp0.50_s1485839278_q100f00_orig_vs200']\n",
    "# obspy_tf_misfit = pickle.loads(Path('results/obspy_tf_misfit.pickle').read_bytes())\n",
    "\n",
    "# # if os.path.isfile('results/obspy_tf_misfit.pickle'):\n",
    "# #     obspy_tf_misfit = pickle.load(open('results/obspy_tf_misfit.pickle', 'rb'))\n",
    "# # else:\n",
    "# dt = 0.04\n",
    "# for model in models:\n",
    "#     if model not in obspy_tf_misfit:\n",
    "#         obspy_tf_misfit[model] = comp_obspy_tf_misfits([model], dt).pop(model)\n",
    "\n",
    "# #     with open('results/obspy_tf_misfit.pickle', 'wb') as fid:\n",
    "# #         pickle.dump(obspy_tf_misfit, fid, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
